{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell first to load custom css\n",
    "from IPython.core.display import HTML\n",
    "HTML(filename=\"./assets/theme.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU_Logo](./assets/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start Workshop to Modern Forecasting\n",
    "\n",
    "## <a name=\"0\">NOTEBOOK 3. DeepAR Forecasting Algorithm in Amazon SageMaker</a>\n",
    "\n",
    "\n",
    "### Forecasting with DeepAR on SageMaker\n",
    "\n",
    "This notebook shows how to prepare a dataset of time series for training DeepAR on SageMaker and how to use the trained model for inference. \n",
    "\n",
    "Amazon SageMaker trains the DeepAR model by randomly sampling training examples from each target time series in the training dataset. Each training example consists of a pair of adjacent context and prediction windows with fixed predefined lengths.\n",
    "\n",
    "For this example we will use the data set [M4-hourly](https://en.wikipedia.org/wiki/Makridakis_Competitions#Fourth_competition,_started_on_January_1,_2018,_ended_on_May_31,_2018), a subset of the [M4 competition](https://www.sciencedirect.com/science/article/pii/S0169207019301128) data set that consists of a 414 time series sampled at 1h intervals. \n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. <a href=\"#1\">Prepare Data</a> \n",
    "2. <a href=\"#2\">Configure SageMaker to Use DeepAR and Train</a> \n",
    "3. <a href=\"#3\">Create Endpoint and Predictor</a>\n",
    "4. <a href=\"#4\">Delete Endpoints</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We install GluonTS to have access to its datasets for this demo. Note that when training on your own dataset, you would not need to install GluonTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet gluonts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import sagemaker\n",
    "import s3fs\n",
    "\n",
    "from gluonts.dataset.repository.datasets import get_dataset, dataset_recipes\n",
    "from gluonts.dataset.util import to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"legend.fontsize\": \"x-large\",\n",
    "    \"figure.titlesize\": \"xx-large\",\n",
    "    \"axes.labelsize\": \"x-large\",\n",
    "    \"axes.titlesize\": \"x-large\",\n",
    "    \"xtick.labelsize\": \"x-large\",\n",
    "    \"ytick.labelsize\": \"x-large\",\n",
    "    \"figure.figsize\": (14, 5),\n",
    "}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\">1. Prepare Data</a>\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [M4-hourly](https://en.wikipedia.org/wiki/Makridakis_Competitions#Fourth_competition,_started_on_January_1,_2018,_ended_on_May_31,_2018) dataset is a subset of the data published for the [M4 competition](https://www.sciencedirect.com/science/article/pii/S0169207019301128). The full dataset for the competition consisted of 100,000 time series sampled at different frequencies. The M4-hourly set contains 414 time series data sampled at 1h intervals. \n",
    "\n",
    "We will download the data using GluonTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(\"m4_hourly\", regenerate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect some of the time series of this data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = 9\n",
    "axes = list(np.ndindex(3, 3))\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, sharex=True, figsize=(16, 16))\n",
    "\n",
    "for i, (train, test, ax) in enumerate(zip(dataset.train, dataset.test, axes.flatten())):\n",
    "\n",
    "    tra = to_pandas(train)\n",
    "    tra.index = tra.index.to_timestamp()\n",
    "    te = to_pandas(test)\n",
    "    te.index = te.index.to_timestamp()\n",
    "    \n",
    "    len_train = len(tra)\n",
    "    ax.plot(te[:len_train], label=\"train\")\n",
    "    ax.plot(te[len_train:], label=\"test\")\n",
    "    fig.autofmt_xdate(rotation=45)\n",
    "    ax.legend()\n",
    "    if i == num_plots:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR JSON input format represents each time series as a JSON object. In the simplest case, each time series just consists of a start time stamp (`start`) and a list of values (`target`). For more complex cases, DeepAR also supports the fields `dynamic_feat` for time series features and `cat` for categorical features, which we will use later.\n",
    "\n",
    "Fortunately, the data downloaded from GluonTS is already in the format needed by SageMaker when it trains the DeepAR algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GluonTS m4-hourly dataset is already in the format required by DeepAR in SageMaker\n",
    "train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the data files locally, let us upload them to S3 where DeepAR can access them. The following cell sets up the S3 path where the m4-hourly data files will be uploaded to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to your desired S3 prefix\n",
    "prefix = \"sagemaker/deepar-sagemaker\"\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "s3_data_path = f\"{bucket}/{prefix}/data\"\n",
    "s3_output_path = f\"{bucket}/{prefix}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell uploads the files from the original location where GluonTS saved them to our desired S3 location so that SageMaker is able to find them when training DeepAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = \"utf-8\"\n",
    "FILE_TRAIN = \"/home/ec2-user/.mxnet/gluon-ts/datasets/m4_hourly/train/data.json.gz\"\n",
    "FILE_TEST = \"/home/ec2-user/.mxnet/gluon-ts/datasets/m4_hourly/test/data.json.gz\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(FILE_TRAIN, bucket, prefix + \"/data/train/train.json.gz\")\n",
    "s3.upload_file(FILE_TEST, bucket, prefix + \"/data/test/test.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"2\">2. Configure SageMaker to Use DeepAR and Train</a>\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We configure the container image to be used for the region that we are running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the estimator that will launch the training job. We can specify the type of instance where the training will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    base_job_name=\"deepar-sagemaker\",\n",
    "    output_path=f\"s3://{s3_output_path}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set the hyperparameters for the training job. For example frequency of the time series used, number of data points the model will look at in the past, number of predicted data points. The other hyperparameters concern the model to train (number of layers, number of cells per layer, likelihood function) and the training options (number of epochs, batch size, learning rate, number of layers, number of cells per layer). Refer to the documentation for a full description of the [available parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html).\n",
    "\n",
    "For `context_length`, the number of time-points that the model gets to see before making the prediction, this value  should be about the same as the `prediction_length`, but it can be modified and made larger as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": \"H\",\n",
    "    \"context_length\": str(dataset.metadata.prediction_length),\n",
    "    \"prediction_length\": str(dataset.metadata.prediction_length),\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"50\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"1e-3\",\n",
    "    \"early_stopping_patience\": \"10\",\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the test data channel, as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test data set. This is done by predicting the last `prediction_length` points of each time series in the test set and comparing this to the actual value of the time series. The computed error metrics will be included as part of the log output.\n",
    "\n",
    "**Note**: the next cell will take a few minutes to complete, depending on data size, model complexity, and training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    \"train\": f\"s3://{s3_data_path}/train/\",\n",
    "    \"test\": f\"s3://{s3_data_path}/test/\",\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"3\">3. Create Endpoint and Predictor</a>\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "**Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.**\n",
    "\n",
    "To query the endpoint and perform predictions, we can define the following utility class `DeepARPredictor`. This class extends the `Predictor` class. Implementing encode and decode functions helps us make requests using `pandas.Series` objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            # serializer=JSONSerializer(),\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=100,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.025\", \"0.1\", \"0.5\", \"0.9\", \"0.975\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(\n",
    "            ts, cat, dynamic_feat, num_samples, return_samples, quantiles\n",
    "        )\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(\n",
    "            res, ts.index.freq, prediction_time, return_samples\n",
    "        )\n",
    "\n",
    "    def __encode_request(\n",
    "        self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles\n",
    "    ):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"]\n",
    "            if return_samples\n",
    "            else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\n",
    "                \"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])\n",
    "            }\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the model and create and endpoint that can be queried using our custom DeepARPredictor class.\n",
    "This cell takes some minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.large\", predictor_cls=DeepARPredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the forecasts that the trained model makes for the first few dataseries of our dataset. We will plot predicted median and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = 9\n",
    "axes = list(np.ndindex(3, 3))\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, sharex=True, figsize=(16, 16))\n",
    "\n",
    "for i, (train, test, ax) in enumerate(zip(dataset.train, dataset.test, axes.flatten())):\n",
    "    \n",
    "    tra = to_pandas(train)\n",
    "    tra.index = tra.index.to_timestamp()\n",
    "    te = to_pandas(test)\n",
    "    te.index = te.index.to_timestamp()\n",
    "\n",
    "    len_train = len(tra)\n",
    "    ax.plot(te[len_train - 100 : len_train], label=\"train\")\n",
    "    ax.plot(te[len_train:], label=\"test\")\n",
    "\n",
    "    # Use our deployed model to make predictions\n",
    "    forecast = predictor.predict(ts=tra)\n",
    "\n",
    "    ax.plot(forecast[\"0.5\"], color=\"yellow\", label=\"DeepAR forecast (median)\")\n",
    "    ax.fill_between(\n",
    "        forecast.index,\n",
    "        forecast[\"0.1\"],\n",
    "        forecast[\"0.9\"],\n",
    "        color=\"b\",\n",
    "        alpha=0.2,\n",
    "        label=\"p80\",\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        forecast.index,\n",
    "        forecast[\"0.025\"],\n",
    "        forecast[\"0.975\"],\n",
    "        color=\"b\",\n",
    "        alpha=0.3,\n",
    "        label=\"p95\",\n",
    "    )\n",
    "\n",
    "    fig.autofmt_xdate(rotation=45)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"upper center\")\n",
    "    if i == num_plots:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect the prediction for one of the time series in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(ts=tra)\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(tra)\n",
    "te[len_train - 150 : len_train].plot(label=\"train\")\n",
    "te[len_train:].plot(label=\"test\")\n",
    "\n",
    "prediction[\"0.5\"].plot(color=\"yellow\", label=\"DeepAR forecast (median)\")\n",
    "plt.fill_between(\n",
    "    prediction.index,\n",
    "    prediction[\"0.1\"],\n",
    "    prediction[\"0.9\"],\n",
    "    color=\"b\",\n",
    "    alpha=0.2,\n",
    "    label=\"p80\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    prediction.index,\n",
    "    prediction[\"0.025\"],\n",
    "    prediction[\"0.975\"],\n",
    "    color=\"b\",\n",
    "    alpha=0.3,\n",
    "    label=\"p95\",\n",
    ")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"4\">4. Delete Endpoints</a>\n",
    "\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Amazon SageMaker stores the trained model in the cloud and provisions the requested infrastructure for its deployment. Upon finishing development and experimentation, one should remember to delete the model and associated endpoints to avoid incurring in unforeseen costs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "In this notebook we've learned how to leverage SageMaker capabilities to train ML models in the cloud. We've used DeepAR for forecasting time series data. This is one of the built-in models available in Amazon SageMaker.  \n",
    "\n",
    "DeepAR is capable of handling multiple time series, creating a single global model. It even has the potential to solve a cold start problem i.e. to generate forecasts for new time series that do not have historical observations but are similar to the ones it has been trained on.\n",
    "\n",
    "Using DeepAR in SageMaker we can focus on experimenting with our time series to get the best possible results, without worrying about the internal infrastructure. We can get the job done very quickly as there is no need to write any training code. All we need to do is prepare the data and do the necessary tunings to refine the model if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
